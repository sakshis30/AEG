{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIZygDZkxGqN"
      },
      "source": [
        "# Installing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wKr3C4SxeOc"
      },
      "source": [
        "Language Check Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zfMzHM6xhYJ"
      },
      "source": [
        "! sudo apt install openjdk-8-jdk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO9RhyIhxr0j"
      },
      "source": [
        "! sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcV6WHpDxxmb"
      },
      "source": [
        "! pip install language-check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM8rEmMsyBSC"
      },
      "source": [
        "Textstat Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXKNA8ohyKwl"
      },
      "source": [
        "pip install textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asBO3bZ0VRec"
      },
      "source": [
        "VaderSentiment Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py7ouewFViXA"
      },
      "source": [
        "pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAgJX3Bu6bcr"
      },
      "source": [
        "Downloading PunktSentenceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A325eIHm6Bbc"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzsCkWP5AyBy"
      },
      "source": [
        "Downloading Stopwords module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3_pYLE9AxBT"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-ZH3esxyRQu"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhJmAp3gyVP0"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import re, collections\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "import language_check\n",
        "import textstat\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHclebpR0a0v"
      },
      "source": [
        "# Importing and Visualising the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DISCgh-e0h55"
      },
      "source": [
        "Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97aF4gAu09T4"
      },
      "source": [
        "dataset = pd.read_excel('training_set_rel3.xls')\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy8pT0P03Sqx"
      },
      "source": [
        "Checking the dimensions of the data i.e. no. of rows and columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7UO46iH3FHK"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRUdTDvO3zZX"
      },
      "source": [
        "Data Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miusMZLV3yT_"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czVbPAio333q"
      },
      "source": [
        "Checking for empty fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8BHpoJg37bj"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfjpysna4Cq8"
      },
      "source": [
        "Finding the number of records for each column for each of the eight essay sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0zTZCIz4HUl"
      },
      "source": [
        "data.groupby('essay_set').agg('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e-0Q_dG4Po7"
      },
      "source": [
        "Copying the contents of data in a new variable to avoid manipulation of the original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDVzAORZ4NaS"
      },
      "source": [
        "temp_data = data[['essay_set','essay','domain1_score']].copy()\n",
        "temp_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Wx8TumWz3o"
      },
      "source": [
        "# Preprocessing : Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLKURZoI8W9N"
      },
      "source": [
        "Cleaning the text using regex function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KCf8l7e7TRL"
      },
      "source": [
        "def process_text(essay):\n",
        "\n",
        "    essay = str(essay)\n",
        "    result = re.sub(r'http[^\\s]*','',essay)  #removing url\n",
        "    result = re.sub('[0-9]+','', result).lower() # remove numbers and lowercase the text\n",
        "    result = re.sub('@[a-z0-9]+', '', result) #Eg: @caps1 will be removed\n",
        "\n",
        "    return re.sub('[%s]*' % string.punctuation, '',result) #remove punctuation\n",
        "\n",
        "temp_data['clean_essay'] = temp_data['essay'].apply(process_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHjOXmZ29k_x"
      },
      "source": [
        "Here, we are using ascii encoding on the string, ignoring the ones that can't be converted and then again decoding it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwRvGyU_9S7h"
      },
      "source": [
        "def decode_essay(essay):\n",
        "\n",
        "    return essay.encode('ascii', 'ignore').decode('ascii')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBVKjiDa_SCH"
      },
      "source": [
        "For Splitting sentences in the paragraph using PunktSentenceTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Nd7H6L_vAp"
      },
      "source": [
        "def tokenize_essay(essay):\n",
        "\n",
        "    strip_essay = essay.strip()\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(strip_essay)\n",
        "    tokenized_sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            tokenized_sentences.append(convert_essay_to_wordlist(raw_sentence))\n",
        "\n",
        "    return tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTWKjRwm-QIt"
      },
      "source": [
        "Tokenizing the sentences to words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjJ4gkkm9kF1"
      },
      "source": [
        "def convert_essay_to_wordlist(sentence):\n",
        "\n",
        "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", sentence)\n",
        "    wordlist = nltk.word_tokenize(clean_sentence)\n",
        "\n",
        "    return wordlist\n",
        "\n",
        "temp_data['clean_essay'] = temp_data['clean_essay'].apply(convert_essay_to_wordlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqlpFTEBCZQS"
      },
      "source": [
        "Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYz1pSqUCEEF"
      },
      "source": [
        "def remove_stopwords(text):\n",
        "\n",
        "    words = [word for word in text if word not in stopwords.words('english')]\n",
        "\n",
        "    return words\n",
        "\n",
        "temp_data['clean_essay'] = temp_data['clean_essay'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eKUKO5UDFyB"
      },
      "source": [
        "def clean_length(token):\n",
        "\n",
        "    return [i for i in token if len(i)>2]\n",
        "\n",
        "temp_data['clean_essay'] = temp_data['clean_essay'].apply(clean_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO1iaMa-FbyN"
      },
      "source": [
        "Calculating Number of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NG7VVJ2E_8g"
      },
      "source": [
        "def sent_count(essay):\n",
        "\n",
        "    sentences = nltk.sent_tokenize(essay)                 #using sent_tokenize to convert paragraph into sentences\n",
        "\n",
        "    return len(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6CWIKC7GR8S"
      },
      "source": [
        "Calculating Number of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxjvcEj3GHhS"
      },
      "source": [
        "def word_count(essay):\n",
        "\n",
        "    clean_essay = re.sub(r'\\W',' ', essay)                 #equivalent to [^a-zA-Z0-9]\n",
        "    words = nltk.word_tokenize(clean_essay)\n",
        "\n",
        "    return len(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OPoTXOwHL1O"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8O47ynqXA7M"
      },
      "source": [
        "Sentiment Analysis using VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3grC3wmXFgF"
      },
      "source": [
        "def sentiment(text):\n",
        "\n",
        "  senti_obj = SentimentIntensityAnalyzer()\n",
        "  sentiment_dict = senti_obj.polarity_scores(text)\n",
        "\n",
        "  return sentiment_dict['pos'], sentiment_dict['neg'], sentiment_dict['neu'], sentiment_dict['compound']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG3ch4gDb2dQ"
      },
      "source": [
        "Flesch Kincaid Grade Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOZlZLHHdisp"
      },
      "source": [
        "def flesch_kincaid_grade(text):\n",
        "\n",
        "  grade = textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "  return grade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9i-RRdzdw1P"
      },
      "source": [
        "Language Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RGm0UqAdy2B"
      },
      "source": [
        "def language_check(text):\n",
        "\n",
        "    error = 0\n",
        "    matches = tool.check(text)\n",
        "    error = error + len(matches)\n",
        "\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}